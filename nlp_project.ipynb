{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T21:04:55.611121Z",
     "start_time": "2023-12-09T21:04:55.609706Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static data:\n",
      "                                               words  \\\n",
      "0  [anyway, ,, thanks, mkm, and, keep, up, the, g...   \n",
      "1  [well, done, steffi, —, keep, up, the, good, w...   \n",
      "2  [please, use, it, as, a, reminder, to, ensure,...   \n",
      "3         [please, do, keep, up, the, good, work, .]   \n",
      "4  [we, hope, you, find, the, current, issue, sti...   \n",
      "\n",
      "                                                tags              candidate  \\\n",
      "0  [O, O, O, O, O, B-IDIOM, I-IDIOM, I-IDIOM, I-I...  keep up the good work   \n",
      "1  [O, O, O, O, B-IDIOM, I-IDIOM, I-IDIOM, I-IDIO...  keep up the good work   \n",
      "2  [O, O, O, O, O, O, O, O, O, O, B-IDIOM, I-IDIO...  keep up the good work   \n",
      "3  [O, O, B-IDIOM, I-IDIOM, I-IDIOM, I-IDIOM, I-I...  keep up the good work   \n",
      "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-I...  keep up the good work   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "Formal data:\n",
      "                                               words  \\\n",
      "0  [‘, you, know, ,, the, panda, who, keeps, an, ...   \n",
      "1  [but, i, will, also, be, keeping, an, eye, on,...   \n",
      "2  [‘, i, will, keep, an, eye, on, him, ,, ’, rea...   \n",
      "3  [moreover, ,, international, operations, direc...   \n",
      "4  [once, duncan, had, managed, to, fire, off, a,...   \n",
      "\n",
      "                                                tags           candidate  \\\n",
      "0  [O, O, O, O, O, O, O, B-IDIOM, I-IDIOM, I-IDIO...  keep [pron] eye on   \n",
      "1  [O, O, O, O, O, B-IDIOM, I-IDIOM, I-IDIOM, I-I...  keep [pron] eye on   \n",
      "2  [O, O, O, B-IDIOM, I-IDIOM, I-IDIOM, I-IDIOM, ...  keep [pron] eye on   \n",
      "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  keep [pron] eye on   \n",
      "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  keep [pron] eye on   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n"
     ]
    }
   ],
   "source": [
    "def read_file(file_path):\n",
    "    \"\"\"Reads a text file and returns a list of lines.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "def preprocess_data(static_words_path, static_tags_path, static_candidates_path,\n",
    "                    formal_words_path, formal_tags_path, formal_candidates_path, formal_labels_path):\n",
    "    # Read data from files\n",
    "    static_words = read_file(static_words_path)\n",
    "    static_tags = read_file(static_tags_path)\n",
    "    static_candidates = read_file(static_candidates_path)\n",
    "\n",
    "    formal_words = read_file(formal_words_path)\n",
    "    formal_tags = read_file(formal_tags_path)\n",
    "    formal_candidates = read_file(formal_candidates_path)\n",
    "    formal_labels = read_file(formal_labels_path)\n",
    "\n",
    "    # Process static idioms - assign label 1 for all instances\n",
    "    static_data = [{'words': words.lower().split(), 'tags': tags.split(),\n",
    "                    'candidate': candidate, 'label': 1}\n",
    "                   for words, tags, candidate in zip(static_words, static_tags, static_candidates)]\n",
    "\n",
    "    # Process formal idioms\n",
    "    formal_data = [{'words': words.lower().split(), 'tags': tags.split(),\n",
    "                    'candidate': candidate, 'label': int(label)}\n",
    "                   for words, tags, candidate, label in zip(formal_words, formal_tags, formal_candidates, formal_labels)]\n",
    "\n",
    "    # Convert to DataFrames for ease of use\n",
    "    static_df = pd.DataFrame(static_data)\n",
    "    formal_df = pd.DataFrame(formal_data)\n",
    "\n",
    "    return static_df, formal_df\n",
    "\n",
    "# File paths for static idioms\n",
    "static_base_path = 'EPIE_Corpus/Static_Idioms_Corpus/'\n",
    "static_words_path = static_base_path + 'Static_Idioms_Words.txt'\n",
    "static_tags_path = static_base_path + 'Static_Idioms_Tags.txt'\n",
    "static_candidates_path = static_base_path + 'Static_Idioms_Candidates.txt'\n",
    "\n",
    "# File paths for formal idioms\n",
    "formal_base_path = 'EPIE_Corpus/Formal_Idioms_Corpus/'\n",
    "formal_words_path = formal_base_path + 'Formal_Idioms_Words.txt'\n",
    "formal_tags_path = formal_base_path + 'Formal_Idioms_Tags.txt'\n",
    "formal_candidates_path = formal_base_path + 'Formal_Idioms_Candidates.txt'\n",
    "formal_labels_path = formal_base_path + 'Formal_Idioms_Labels.txt'\n",
    "\n",
    "# Preprocess and combine the data\n",
    "static_df, formal_df = preprocess_data(static_words_path,\n",
    "                                       static_tags_path,\n",
    "                                       static_candidates_path,\n",
    "                                       formal_words_path,\n",
    "                                       formal_tags_path,\n",
    "                                       formal_candidates_path,\n",
    "                                       formal_labels_path)\n",
    "print(\"Static data:\")\n",
    "print(static_df.head())\n",
    "print(\"Formal data:\")\n",
    "print(formal_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:42:44.219464Z",
     "start_time": "2023-12-10T00:42:44.033158Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "label\n1    2761\n0     375\nName: count, dtype: int64"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formal_df[\"label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:42:48.830531Z",
     "start_time": "2023-12-10T00:42:48.825729Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11957908163265306\n"
     ]
    }
   ],
   "source": [
    "print(375 / (375 + 2761))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:42:49.939518Z",
     "start_time": "2023-12-10T00:42:49.930186Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The formal dataset contains 12% literal uses and 88% idiomatic uses. The static dataset only contains idiomatic uses. For the first experiment, we will train a classifier on the formal dataset to determine whether a sentence is formal or idiomatic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "# Concatenate the words into a single string for each sample\n",
    "formal_df['text'] = formal_df['words'].apply(lambda x: ' '.join(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:42:54.859670Z",
     "start_time": "2023-12-10T00:42:54.855048Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    2209\n",
      "0     299\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Random Forest Scores:\n",
      "accuracy: Mean=0.89, Std=0.00\n",
      "precision_macro: Mean=0.83, Std=0.08\n",
      "recall_macro: Mean=0.54, Std=0.01\n",
      "f1_macro: Mean=0.54, Std=0.02\n",
      "\n",
      "\n",
      "SVM Scores:\n",
      "accuracy: Mean=0.89, Std=0.00\n",
      "precision_macro: Mean=0.80, Std=0.04\n",
      "recall_macro: Mean=0.53, Std=0.01\n",
      "f1_macro: Mean=0.53, Std=0.02\n",
      "\n",
      "\n",
      "Logistic Regression Scores:\n",
      "accuracy: Mean=0.87, Std=0.02\n",
      "precision_macro: Mean=0.68, Std=0.04\n",
      "recall_macro: Mean=0.66, Std=0.03\n",
      "f1_macro: Mean=0.67, Std=0.04\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_formal = formal_df[\"text\"]\n",
    "y_formal = formal_df[\"label\"]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train_formal, X_test_formal, y_train_formal, y_test_formal = train_test_split(X_formal, y_formal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the class distribution before resampling\n",
    "print(pd.Series(y_train_formal).value_counts())\n",
    "print()\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "# Initialize classifiers\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "svm_clf = SVC(random_state=42)\n",
    "lr_clf = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create a pipeline that first vectorizes the text and then oversamples the minority class before training\n",
    "# Note: Oversampling and vectorization are done inside the cross-validation loop to avoid data leakage\n",
    "rf_pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('oversample', RandomOverSampler(random_state=42)),\n",
    "    ('classifier', rf_clf)\n",
    "])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('oversample', RandomOverSampler(random_state=42)),\n",
    "    ('classifier', svm_clf)\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline([\n",
    "    ('tfidf', vectorizer),\n",
    "    ('oversample', RandomOverSampler(random_state=42)),\n",
    "    ('classifier', lr_clf)\n",
    "])\n",
    "\n",
    "# Create a dictionary of pipelines and classifier types for ease of reference\n",
    "pipelines = {\n",
    "    \"Random Forest\": rf_pipeline,\n",
    "    \"SVM\": svm_pipeline,\n",
    "    \"Logistic Regression\": lr_pipeline\n",
    "}\n",
    "\n",
    "scoring_metrics = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "# Perform 5-fold cross-validation and print the results\n",
    "for classifier_name, pipeline in pipelines.items():\n",
    "    scores = cross_validate(pipeline, X_train_formal, y_train_formal, cv=5, scoring=scoring_metrics)\n",
    "    print(f'{classifier_name} Scores:')\n",
    "    for metric_name in scoring_metrics:\n",
    "        metric_scores = scores[f'test_{metric_name}']\n",
    "        print(f'{metric_name}: Mean={metric_scores.mean():.2f}, Std={metric_scores.std():.2f}')\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:43:07.669231Z",
     "start_time": "2023-12-10T00:42:59.388231Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "# Vectorize the data\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train_formal)\n",
    "X_test_vectorized = vectorizer.transform(X_test_formal)\n",
    "\n",
    "# Fit the models on the training set\n",
    "rf_clf.fit(X_train_vectorized, y_train_formal)\n",
    "svm_clf.fit(X_train_vectorized, y_train_formal)\n",
    "lr_clf.fit(X_train_vectorized, y_train_formal)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_clf.predict(X_test_vectorized)\n",
    "y_pred_svm = svm_clf.predict(X_test_vectorized)\n",
    "y_pred_lr = lr_clf.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate the confusion matrices\n",
    "conf_matrix_rf = confusion_matrix(y_test_formal, y_pred_rf)\n",
    "conf_matrix_svm = confusion_matrix(y_test_formal, y_pred_svm)\n",
    "conf_matrix_lr = confusion_matrix(y_test_formal, y_pred_lr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:43:20.253381Z",
     "start_time": "2023-12-10T00:43:18.737056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for Random Forest:\n",
      "[[ 10  66]\n",
      " [  4 548]]\n",
      "\n",
      "Confusion Matrix for SVM:\n",
      "[[  3  73]\n",
      " [  1 551]]\n",
      "\n",
      "Confusion Matrix for Logistic Regression:\n",
      "[[  2  74]\n",
      " [  0 552]]\n"
     ]
    }
   ],
   "source": [
    "# Print the confusion matrices\n",
    "print(\"Confusion Matrix for Random Forest:\")\n",
    "print(conf_matrix_rf)\n",
    "print(\"\\nConfusion Matrix for SVM:\")\n",
    "print(conf_matrix_svm)\n",
    "print(\"\\nConfusion Matrix for Logistic Regression:\")\n",
    "print(conf_matrix_lr)\n",
    "\n",
    "# Note: The confusion matrix format is\n",
    "# [[true_negative, false_positive],\n",
    "#  [false_negative, true_positive]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:43:20.773535Z",
     "start_time": "2023-12-10T00:43:20.767572Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "label\n1    552\n0     76\nName: count, dtype: int64"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_formal.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:43:24.978569Z",
     "start_time": "2023-12-10T00:43:24.972438Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All the models substantially underpredicted the negative samples. The random forest classifier predicted the most negative samples, but it still had a reasonably high error rate even for the ones it did predict as negative - only 10 out of the 14 samples it predicted as negative were actually negative."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_rf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:43:27.306649Z",
     "start_time": "2023-12-10T00:43:27.300944Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_formal[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:43:28.640791Z",
     "start_time": "2023-12-10T00:43:28.635910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some false positive samples:\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "iLocation based boolean indexing on an integer type is not available",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[83], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Look at some of the false positives\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome false positive samples:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mX_test_formal\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfp_rf\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mhead())\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Look at some of the false negatives\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSome false negative samples:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/work/platform/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1153\u001B[0m, in \u001B[0;36m_LocationIndexer.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   1150\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxis \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   1152\u001B[0m maybe_callable \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[0;32m-> 1153\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_getitem_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmaybe_callable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/work/platform/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1700\u001B[0m, in \u001B[0;36m_iLocIndexer._getitem_axis\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1697\u001B[0m     key \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(key)\n\u001B[1;32m   1699\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m com\u001B[38;5;241m.\u001B[39mis_bool_indexer(key):\n\u001B[0;32m-> 1700\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_key\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1701\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getbool_axis(key, axis\u001B[38;5;241m=\u001B[39maxis)\n\u001B[1;32m   1703\u001B[0m \u001B[38;5;66;03m# a list of integers\u001B[39;00m\n",
      "File \u001B[0;32m~/work/platform/.venv/lib/python3.11/site-packages/pandas/core/indexing.py:1540\u001B[0m, in \u001B[0;36m_iLocIndexer._validate_key\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(key, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key\u001B[38;5;241m.\u001B[39mindex, Index):\n\u001B[1;32m   1539\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39minferred_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minteger\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1540\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m   1541\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miLocation based boolean \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1542\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindexing on an integer type \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1543\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis not available\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1544\u001B[0m         )\n\u001B[1;32m   1545\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1546\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miLocation based boolean indexing cannot use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1547\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man indexable as a mask\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1548\u001B[0m     )\n\u001B[1;32m   1549\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: iLocation based boolean indexing on an integer type is not available"
     ]
    }
   ],
   "source": [
    "# Extract false positives and false negatives for analysis\n",
    "fp_rf = (y_pred_rf == 1) & (y_test_formal == 0)\n",
    "fn_rf = (y_pred_rf == 0) & (y_test_formal == 1)\n",
    "\n",
    "# Look at some of the false positives\n",
    "print(\"Some false positive samples:\")\n",
    "print(X_test_formal.iloc[fp_rf].head())\n",
    "\n",
    "# Look at some of the false negatives\n",
    "print(\"Some false negative samples:\")\n",
    "print(X_test_formal.iloc[fn_rf].head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:43:39.142212Z",
     "start_time": "2023-12-10T00:43:39.120130Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "1791    0\n528     1\n449     0\n2994    1\n3068    1\n       ..\n1037    1\n1703    0\n969     1\n1288    1\n2298    1\nName: label, Length: 628, dtype: int64"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_formal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:44:53.789410Z",
     "start_time": "2023-12-10T00:44:53.786364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "0       True\n1      False\n2       True\n3      False\n4      False\n       ...  \n623    False\n624    False\n625    False\n626    False\n627    False\nName: label, Length: 628, dtype: bool"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_rf.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T00:45:38.817626Z",
     "start_time": "2023-12-10T00:45:38.814582Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "1791     True\n528     False\n449      True\n2994    False\n3068    False\n        ...  \n1037    False\n1703    False\n969     False\n1288    False\n2298    False\nName: label, Length: 628, dtype: bool"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_rf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T01:08:16.660020Z",
     "start_time": "2023-12-10T01:08:16.656444Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "              candidate  \\\n2711    tie [pron] knot   \n291             bad egg   \n325     be in hot water   \n2279  lose [pron] touch   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                              text  \n2711                                                                                                                                                                                                                                                                                                                                                                                                          stefan ties the knot  \n291                                                                                                                                                                                                                                                                                                                                                                                                              he 's a bad egg .  \n325   i 'm in hot water over various things — nothing important enough to bore you with , but i know the director wants shot of me — he 's said as much and i 'm sure my days are numbered ; i feel like i 'm struggling to keep my head above water — oh , i knew this particular pool was going to be deep , but he 's there waiting just above the surface to shove me under , finding some half-baked reason to be rid of me .  \n2279                                                                                                                                                                                                                                                                well away from the beaten track he laid her gently on a bed of moss and bracken , and she opened her arms to him , loath to lose his touch for even a second .  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>candidate</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2711</th>\n      <td>tie [pron] knot</td>\n      <td>stefan ties the knot</td>\n    </tr>\n    <tr>\n      <th>291</th>\n      <td>bad egg</td>\n      <td>he 's a bad egg .</td>\n    </tr>\n    <tr>\n      <th>325</th>\n      <td>be in hot water</td>\n      <td>i 'm in hot water over various things — nothing important enough to bore you with , but i know the director wants shot of me — he 's said as much and i 'm sure my days are numbered ; i feel like i 'm struggling to keep my head above water — oh , i knew this particular pool was going to be deep , but he 's there waiting just above the surface to shove me under , finding some half-baked reason to be rid of me .</td>\n    </tr>\n    <tr>\n      <th>2279</th>\n      <td>lose [pron] touch</td>\n      <td>well away from the beaten track he laid her gently on a bed of moss and bracken , and she opened her arms to him , loath to lose his touch for even a second .</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_positives_rf = formal_df.iloc[fp_rf[fp_rf].index].drop(columns=[\"words\", \"tags\", \"label\"], axis=1)\n",
    "false_negatives_rf = formal_df.iloc[fn_rf[fn_rf].index].drop(columns=[\"words\", \"tags\", \"label\"], axis=1)\n",
    "false_negatives_rf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T01:15:50.783505Z",
     "start_time": "2023-12-10T01:15:50.778387Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some false positive samples for Random Forest:\n",
      "- 1791: adding fuel to the fire is deepa kaur , the daughter of rajah man singh , the maharajah 's late brother . (add fuel to [pron] fire)\n",
      "- 449: paul bit his lip as he jogged at his father 's shoulder along a trail that wound through fringes of jungle beside the la nga river . (bite [pron] lip)\n",
      "- 1146: so as i can take her for a ride and back . (take for [pron] ride)\n",
      "- 567: candles were brought to the tables and any mishaps in the food or service were blamed on the lack of power , which lasted for three hours . (bring to [pron] table)\n",
      "- 2922: keep them cool (keep [pron] cool)\n",
      "- 2124: your partner standing behind you , pulls your leg towards him gently . (pull [pron] leg)\n",
      "- 1814: the axeman wore the first beard of a boy and had big ears that stuck through his hair . (have big ears)\n",
      "- 2285: ‘ good , ’ said caspar , meaning it , and fenella , still curled into her uncomfortable corner , crossed her fingers and tried not to disturb the shining folds of the robes all about her . (cross [pron] fingers)\n",
      "- 1159: you have never taken us on a ride on your motorbike this year . (take for [pron] ride)\n",
      "- 433: so please , please could you tell the people concerned ( martin ‘ i down everything ’ hill and ross ‘ i have an extremely big head ’ nicholls ) that the c64 is not crap at all , in fact it 's very very good ! (big head)\n",
      "\n",
      "Some false negative samples for Random Forest:\n",
      "- 2711: stefan ties the knot (tie [pron] knot)\n",
      "- 291: he 's a bad egg . (bad egg)\n",
      "- 325: i 'm in hot water over various things — nothing important enough to bore you with , but i know the director wants shot of me — he 's said as much and i 'm sure my days are numbered ; i feel like i 'm struggling to keep my head above water — oh , i knew this particular pool was going to be deep , but he 's there waiting just above the surface to shove me under , finding some half-baked reason to be rid of me . (be in hot water)\n",
      "- 2279: well away from the beaten track he laid her gently on a bed of moss and bracken , and she opened her arms to him , loath to lose his touch for even a second . (lose [pron] touch)\n"
     ]
    }
   ],
   "source": [
    "def format_samples(samples_series):\n",
    "    return \"\\n\".join([f\"- {i}: {sample['text']} ({sample['candidate']})\" for i, sample in samples_series.iterrows()])\n",
    "\n",
    "# Set the option to display the full content of the column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Print some of the false positives and false negatives for Random Forest\n",
    "print(\"Some false positive samples for Random Forest:\")\n",
    "print(format_samples(false_positives_rf.head(n=10)))\n",
    "\n",
    "print(\"\\nSome false negative samples for Random Forest:\")\n",
    "print(format_samples(false_negatives_rf.head(n=10)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T01:23:09.412466Z",
     "start_time": "2023-12-10T01:23:09.411183Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "data": {
      "text/plain": "label\n1    21891\nName: count, dtype: int64"
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_df[\"label\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T02:12:23.462548Z",
     "start_time": "2023-12-10T02:12:23.458662Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taking a closer look at these samples, we learn some important pieces of information:\n",
    "- Some of the samples are mislabeled. For example, index 1791 is an idiomatic usage of the phrase \"add fuel to the fire,\" but the sentence is labeled `0`. Index 2279 is a literal usage of the phrase \"lose one's touch,\" but it is labeled as `1`. Even though the labels for these samples are incorrect, `translated_sentences.txt` has the correct translations for them. According to the paper, \"These labels are done by automatic systems with high accuracy.\" The paper does not mention the source of the translated sentences. The original sentences are taken from StringNet, which is a knowledge base containing n-grams extracted from the British National Corpus (http://nav.stringnet.org/about.php). From looking at the StringNet Navigator, it's unclear if the translated sentences could have come from there.\n",
    "- For some of the samples, it's hard to tell if a particular phrase is used idiomatically or literally due to the lack of context. For example, the sample at index 2711 is simply \"Stefan ties the knot.\" This could be referring to getting married, which would be the idiomatic sense of the phrase, or it could be referring to literally tying a knot (in a piece of rope or similar). Unfortunately, the dataset does not provide the context in which a sentence occurs, and going to the original source of the sentences (StringNet) does not provide any additional help in determining the context of a phrase, as mentioned above.\n",
    "\n",
    "Due to these challenges, as well as the fact that the dataset contains a relatively small number of sentences with literal uses of phrases (375 negative samples of sentences with formal idioms; 2761 positive samples of sentences with formal idioms; 21891 samples of static idioms, all of which are positive), we will focus on sequence labeling for the remainder of this report. The dataset has more data that is suitable for sequence labeling than for idiom classification. Additionally, the original study that published the data discusses its potential utility in sequence labeling and trains a sequence labeling on the dataset, which can be used as a benchmark for our sequence labeling model."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
